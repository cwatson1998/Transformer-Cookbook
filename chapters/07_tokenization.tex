%-----------------------------------
%
\chapter{Tokenization}
%
%-----------------------------------

\section{Intro}


Tokenization is a step in natural language processing (NLP) that splits text into smaller pieces called \textit{tokens}.
These tokens are the basic units of data processed by language models, and different tokenization strategies have been explored in literature.
For instance, consider the phrase \texttt{"I love NLP"}, which may be split by different tokenization schemes as follows:
\begin{itemize}
    \item \textbf{Word-level:}: \texttt{["I", "love", "NLP"]}

    \item \textbf{Character-level:} \texttt{["I", " ", "l", "o", "v", "e", " ", "N", "L", "P"]}

    \item \textbf{Subword-level:} \texttt{["I", "lo", "ve", "NL", "P"]} or \texttt{["I", "love", "N", "LP"]}

\end{itemize}
In standard NLP pipelines, an embedding function then maps each token into a vector in \(\mathbb{R}^{d}\), where \(d\) is called the \textit{embedding dimension}, \textit{hidden dimension}, or \textit{latent dimension}.
In GPT-2, for instance, one might have \(d = 768\)~\citep{radford2019language}, and in Llama-based models, we may have \(d = 4096\)~\citep{touvron2023llama}.
A simplified but representative text-to-vector pipeline is as follows:
\begin{equation*}
    \underbrace{\texttt{"I love NLP"}}_{\text{input text}}
    \xrightarrow{\text{Tokenizer}}
    \underbrace{\texttt{["I", "love", "NLP"]}}_{\text{tokens}}
    \xrightarrow{\text{Embed}}
    \underbrace{
    \begin{bmatrix}
        0.94 & \cdots & - 1.60 \\
        - 0.57 & \cdots & 2.58 \\
        0.34 & \cdots & 0.05
    \end{bmatrix}
}_{\text{embedded inputs in \(\mathbb{R}^{3 \times d}\)}}
\end{equation*}

In the above, we have mapped each token to a vector in \(\mathbb{R}^{d}\) and laid them out horizontally in a single matrix following convention, where the numbers were arbitrarily chosen for illustrative purposes.
We emphasize that the above is a simplified presentation of a complex process, and we refer to~\citep{mielke2021between} for a comprehensive survey and history of various tokenization methods in NLP.



For theoretical analyses of transformers, one usually assumes that a vectorized embedding of the text is given, i.e., the input text is already processed by some tokenization method and embedding function.
This is because tokenization and embedding strategies are often model and task-specific, and so are commonly abstracted as a pre-processing step.
Although this simplification is convenient for theoretical analysis, the details of tokenizations and embeddings should be considered in practical implementations.





\section{Tokenization Methods}

\subsection{Byte Pair Encoding (BPE)}

\AX{Maybe will write}

\section{Extra Symbols}

\subsection{BOS}\label{sec:BOS}

The use of a special symbol such as BOS (beginning of sequence), EOS (end of sequence), and CLS (classification) These are special symbols outside of the usual alphabet of symbols. These special symbols are often used as distinguished positions to carry information about the input - for instance, one might take the output at the CLS position as an embedding of the entire input sentence. Having one of these tokens in the sequence also serves the functional purpose of allowing a transformer to create the value $\frac{1}{n}$ (or $\frac{1}{i+1}$ if future-masked) by using uniform attention \cref{sec:uniform-attention}. For instance, constructing $\frac{1}{i+1}$ was used in \citet{merrill-sabharwal-2024-cot} to do retrieval using the layer-norm hash.

\subsection{Turing Machine Proof?}

\AS{Will write}


\section{Notes to be deleted}
\AY{We still need to talk about to what extent tokenization will be discussed}

\uvp{Some scoping decisions: tokenizer vocab creation methods? Tokenizer application (inference) methods? Special tokens?}

\AY{add definitions, mention use of extra symbols eg for turing machine proofs, BOS and EOS}

\NS{test}
