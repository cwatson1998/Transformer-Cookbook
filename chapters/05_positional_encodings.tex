%-----------------------------------
%
\chapter{Positional Encodings}
%
%-----------------------------------

Transformers most often incorporate positional encodings as a method for injecting information about the positions of tokens into attention operations, which do not have an inherently represention of token ordering. The original transformer model in \citet{vaswani-etal-2017-attention} used sinusoidal positional encodings, but since then, many works have proposed other encodings addressing length generalization \citep{kazemnejad2024impact}, relative distances between tokens \citep{shaw2018self}, as well as representing tree structure in sequences \citep{shiv2019}. In this section we focus on the theoretical aspects of positional encodings---how they may affect the expressiveness of the model.


\section{No Positional Encoding}

\subsection{$\frac{1}{i}$}

Obtaining the value $\frac{1}{i}$ at position $i$ can be introduced via a positional encoding, or it can be computed using a future-masked uniform attention layer \cref{sec:uniform-attention} and the presence of a beginning-of-sequence token (as in \cref{sec:BOS}). This positional encoding plays a role in the constructions of \citet{barcelo-etal-2024-logical}, \citet{merrill-sabharwal-2024-cot}, and others. Additionally, as described in \cref{sec:tie-breaking}, this value may be used to simulate a $\UHAT$ using an $\AHAT$.



\subsection{$\frac{i}{n}$}
Similar to above, the value $\frac{i}{n}$ may also be obtained at position $i$ using using an unmasked uniform attention layer \cref{sec:uniform-attention} and the presence of a beginning-of-sequence token. This positional encoding can be found in the constructions of \cite{merrill2023parallelism,chiang-cholak-2022-parity,strobl2024transformers}.

\section{Other Powers of $i$}

One recurring theme is using four consecutive powers of $i$, for example, $1/i^2, 1/i, 1, i$. It's not always the same powers of $i$. No idea why four are needed.


\section{Sinusoidal Positional Encoding}

We present the sinusoidal position encodings as was introduced in~\citet{vaswani-etal-2017-attention}.
For embedding dimension \(1 \leq i \leq d\) and position \(p\), let:
\begin{align*}
    \msf{PE}(p, 2i) = \sin\left(\frac{p}{M^{2i/d}}\right),
    \quad
    \msf{PE}(p, 2i+1) = \cos\left(\frac{p}{M^{2i/d}}\right)
\end{align*}
where \(M\) is a large number, such as \(M = 10000\) in~\citet{vaswani-etal-2017-attention}.


\section{Learned Positional Encoding}

Another option for positional encodings is to assign learnable parameter vectors to each position up to the maximum context length $n$ \citep{vaswani-etal-2017-attention}, though this strategy is rarely used in newer LMs.

From the perspective of formal language expressivity (where behavior over arbitrary-length strings matters), learned positional encodings are difficult to model because the vectors are not very uniform as a function of position, in contrast to $1/i, i/n$, or sinusoidal encodings.
Technically, when learned embeddings are used in practice, the function guaranteeing uniformity of the embeddings is the computation that trains the model, though this is typically abstracted away in theoretical analyses.
If, instead, the positional embeddings are allowed to be any function of the position, then the model is capable of expressing super-Turing computation similar to nonuniform $\mathsf{TC}^0$ circuits.

\section{Other Practical Choices}

\subsection{Relative Positional Encoding}

Relative positional encodings contain pairwise information about the positions of tokens, as opposed to their absolute position in the sequence, and has shown to improve generalisation.
It was formulated by \citet{shaw2018self}, modifying the keys and values as in Equation~\ref{eq:relative-key-keys} and Equation~\ref{eq:relative-key-values}, where the vectors $a^V_{ij}, a^K_{ij} \in \R^{d_z}$ represent the edge between input elements $x_i$ and $x_j$.

\begin{equation}
    \label{eq:relative-key-keys}
    e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^\top}{\sqrt{d_z}}
\end{equation}

\begin{equation}
    \label{eq:relative-key-values}
    z_i = \sum_{j = 1}^n \alpha_{ij} (x_j W^V + a_{ij}^V)
\end{equation}

\citet{huang2020improvetransformermodelsbetter} extend this approach in Equation~\ref{eq:relative-key-query}, with their fourth method, to foster interaction between query, key and relative positional embeddings.

\begin{equation}
    \label{eq:relative-key-query}
    e_{ij} = \frac{(x_i W^Q + a_{ij}^K)(x_j W^K + a_{ij}^K)^T - \langle a_{ij}^K, a_{ij}^K\rangle}{\sqrt{d_z}}
\end{equation}

\AY{maybe allows hard attention to constant offset eg i-1}


\subsection{Rotary Positional Encoding}

The Rotary Positional Encoding (RoPE)~\citep{su2024roformer} is a form of relative positional encoding that has been used in LLMs such as Llama~\citep{touvron2023llama}.
The idea is to apply a position-dependent rotation to each embedded token \(x \in \mathbb{R}^d\) at each position \(p\).
Specifically, RoPE applies a 2-dimensional rotation matrix \(R_{p,i} \in \mathbb{R}^{2 \times 2}\) at consecutive coordinate pairs \(x_{2i-1, 2i}\), for \(i = 1, \ldots, d/2\), where we assume that \(d\) is even.
More explicitly, the RoPE encoding of \(x \in \mathbb{R}^{d}\) at position \(p\) is expressed as follows:
\begin{equation}
\mathsf{RoPE}(x, p)
    = \left[\begin{array}{c|c|c}
            R_{p,1} & & \\
            \hline
            & \ddots & \\
            \hline
            & & R_{p, d/2}
        \end{array}\right]
        \begin{bmatrix}
            x_1 \\ x_2 \\
            \hline \vdots \\
            \hline
            x_{d-1} \\ x_{d}
        \end{bmatrix},
        \quad R_{p,k} = \begin{bmatrix}
            \cos (p \theta_k) & -\sin (p \theta_k) \\
            \sin (p \theta_k) & \cos (p \theta_k)
        \end{bmatrix},
\end{equation}
where the rotation angle is \(\theta_k = M^{-2(k-1)/d}\), with constant \(M = 10000\) as originally used in~\citep{su2024roformer}.
The definition of RoPE above means that the relative position of tokens uniquely determines the relative angle of their embedding rotations.

\subsection{ALiBi Positional Encoding}

\citet{press2022train} introduce ALiBi: a relative position encoding strategy that biases attention heads towards more recent positions.
This is implemented by adding a head-specific bias to the attention scores:

\begin{equation*}
    \mathrm{softmax}\left( \mathbf q_i \mathbf K^\top + m \cdot \langle -(i-1), \ldots, -1, 0 \rangle \right)V
\end{equation*}

It is unclear how this affects the expressivity of the transformer compared to standard attention.
One expressivity issue with ALiBI is that uniform attention is not possible.
However, if the bias is learned or some heads have no bias, then this issue circumvented.\footnote{\url{https://x.com/CFGeek/status/1754198897629753421}}.
