%-----------------------------------
%
\chapter{Basic Ingredients}
%
%-----------------------------------

\AY{Introduce basic ingredients and the most rudimentry of constructions. Boolean representations, residual stream, routing, etc}

\uvp{Training practice? Masking strategies? \textleftarrow{} this has some very interesting theoretical discussions around it}

\section{Boolean Representations}
\label{sec:booleans}

A very common ingredient is truth (Boolean) values, and there are many choices for how to represent them.
There are two main issues to think about: how the Boolean operators are defined, and how they interact with layer normalization.

\paragraph{Boolean operations} are covered in detail in \cref{sec:ffnn_boolean}. We want them to be continuous functions, because transformers are continuous functions. This rules out $\text{false}=0, \text{true}>0$, because negation is not continuous.
And sometimes, we want Boolean $\lor$ and $\land$ to be computed using arithmetic $+$ and $\times$ (namely, when using a dot-product to simulate disjunctive normal form, \cref{sec:att_dnf}). We can get this with
\begin{align*}
  \text{false} &= 0 &\text{true} &\ge 1.
\end{align*}

\paragraph{Layer normalization} is covered in detail in \cref{sec:layernorm}. Usually, we want layer normalization to preserve truth values (false stays false, true stays true). Two schemes to do this are:
\begin{itemize}
\item If the components of a feature vector have \emph{fixed mean and variance}, then we can set the layer normalization to the same mean and variance, and it will have no effect. Representations that have this property are
  \begin{align*}
    \text{true} &= (0, 1) & \text{false} &= (1, 0) \\
    \intertext{or}
    \text{true} &= (-1, +1) & \text{false} &= (+1, -1).
  \end{align*}
\item However, sometimes we can't guarantee fixed mean and variance (for example, because the feature vectors also contain integer representations, \cref{sec:integers}). If the components of a feature vector have \emph{zero mean} and are \emph{scale-invariant}, that is, $\mathbf{v}$ and $k\mathbf{v}$ have the same meaning, then we can set the layer normalization to have zero mean and any variance, and it will not change the meaning of anything. A representation that has this property is
  \begin{align*}
    \text{true} &= (-\delta, +\delta) \quad (\delta>0) & \text{false} &= (+\delta, -\delta) \quad (\delta>0).
  \end{align*}
\end{itemize}

Unfortunately, there doesn't seem to be any one representation that has all the properties we want. It may be necessary to switch between representations as needed.

\section{Integer Representations}
\label{sec:integers}


Integer values are another common ingredient, often used to represent counts or indices. The same issues arise as with Boolean values, but furthermore the sign and magnitude of the integers can be important, as we may need to add and compare them.

A common choice is to represent an integer $C\in \Z$ as

  \begin{align*}
    \frac{C}{i} & & \text{where $i$ is the current position where this integer is stored}\\
  \end{align*}

In constructions, we often end up with a denominator of $i$ when using future-masked uniform-attention to compute integer values.
Without masking, we may see $C/n$ where $n$ is the length of the input \cite{chiang+:icml2023}.
This representation also has the zero-mean variant of $(-C/i, +C/i)$.

Since each position has the same denominator, position-wise operations are fairly straightforward to implement using this representation. Position-wise addition and subtraction using this representation is straightforward, and described in \cref{sec:ffnn_addition}. Position-wise comparison of integers is more complicated, as it depends on the Boolean representation used as well as the presence of layernorm, but is described in \cref{sec:assembly_comparison}.

Since different positions have different denominators,  binary operations over integers in different positions is tricky using this representation.
\AY{define layernorm hash and selective layernorm}

\cref{sec:ln_hash}

