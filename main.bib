

@inproceedings{vaswani-etal-2017-attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS)},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@article{su2024roformer,
  title     = {Roformer: Enhanced transformer with rotary position embedding},
  author    = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal   = {Neurocomputing},
  volume    = {568},
  pages     = {127063},
  year      = {2024},
  publisher = {Elsevier}
}

@inproceedings{arora+:2018,
  author    = {Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee},
  title     = {Understanding Deep Neural Networks with Rectified Linear Units},
  year      = 2018,
  booktitle = {Proceedings of the Sixth International Conference on Learning Representations (ICLR)},
  url       = {https://openreview.net/forum?id=B1J_rgWRW}
}

@inproceedings{feng2024towards,
  title     = {Towards Revealing the Mystery behind {C}hain of {T}hought: A Theoretical Perspective},
  author    = {Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
  year      = {2023},
  url       = {https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html}
}

@inproceedings{akyurek2022learning,
  title     = {What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author    = {Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle = {The Eleventh International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://openreview.net/forum?id=0g0X4H8yN4I}
}

@misc{hendrycks2023,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Dan Hendrycks and Kevin Gimpel},
  year          = {2023},
  eprint        = {1606.08415},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  doi           = {10.48550/arXiv.1606.08415}
}

@article{Fukushima1975,
  author   = {Fukushima, Kunihiko},
  title    = {Cognitron: A self-organizing multilayered neural network},
  journal  = {Biological Cybernetics},
  year     = {1975},
  month    = {Sep},
  day      = {01},
  volume   = {20},
  number   = {3},
  pages    = {121-136},
  abstract = {A new hypothesis for the organization of synapses between
              neurons is proposed: ``The synapse from neuron x to neuron y is
              reinforced when x fires provided that no neuron in the vicinity
              of y is firing stronger than y''. By introducing this hypothesis,
              a new algorithm with which a multilayered neural network is
              effectively organized can be deduced. A self-organizing
              multilayered neural network, which is named ``cognitron'', is
              constructed following this algorithm, and is simulated on a
              digital computer. Unlike the organization of a usual brain models
              such as a three-layered perceptron, the self-organization of a
              cognitron progresses favorably without having a ``teacher'' which
              instructs in all particulars how the individual cells respond.
              After repetitive presentations of several stimulus patterns, the
              cognitron is self-organized in such a way that the receptive
              fields of the cells become relatively larger in a deeper layer.
              Each cell in the final layer integrates the information from
              whole parts of the first layer and selectively responds to a
              specific stimulus pattern or a feature.},
  issn     = {1432-0770},
  doi      = {10.1007/BF00342633},
  url      = {https://doi.org/10.1007/BF00342633}
}

@inproceedings{chiang-cholak-2022-parity,
  title      = {Overcoming a Theoretical Limitation of Self-Attention},
  author     = {Chiang, David  and
                Cholak, Peter},
  booktitle  = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  month      = may,
  year       = {2022},
  xaddress   = {Dublin, Ireland},
  xpublisher = {Association for Computational Linguistics},
  url        = {https://aclanthology.org/2022.acl-long.527},
  doi        = {10.18653/v1/2022.acl-long.527},
  pages      = {7654--7664}
}

@inproceedings{merrill-sabharwal-2024-cot,
  author    = {William Merrill and Ashish Sabharwal},
  year      = 2024,
  title     = {The Expressive Power of Transformers with Chain of Thought},
  booktitle = {Proceedings of the Twelfth International Conference on Learning Representations (ICLR)},
  arxiv_url = {https://arxiv.org/abs/2310.07923},
  url       = {https://openreview.net/forum?id=NjNGlPh8Wh},
  pdf       = {https://openreview.net/pdf?id=NjNGlPh8Wh}
}

@inproceedings{barcelo-etal-2024-logical,
  title     = {Logical Languages Accepted by Transformer Encoders with Hard Attention},
  author    = {Pablo Barcel{\'o} and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir Podolskii},
  year      = 2024,
  booktitle = {Proceedings of the Twelfth International Conference on Learning Representations (ICLR)},
  arxiv_url = {https://arxiv.org/abs/2310.03817},
  url       = {https://openreview.net/forum?id=gbrHZq07mq},
  pdf       = {https://openreview.net/pdf?id=gbrHZq07mq}
}

% Note that the version below has an error that results in omitting the proof that Majority can be recognized by a Transformer
@article{perez-etal-2021-turing,
  author    = {Jorge P{\'{e}}rez and
               Pablo Barcel{\'{o}} and
               Javier Marinkovic},
  title     = {Attention is {T}uring-Complete},
  journal   = {Journal of Machine Learning Research},
  volume    = {22},
  pages     = {75:1--75:35},
  year      = {2021},
  url       = {http://jmlr.org/papers/v22/20-302.html},
  timestamp = {Mon, 31 Jan 2022 17:23:36 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/PerezBM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chiang+:icml2023,
  xurl      = {https://arxiv.org/abs/2301.10743},
  url       = {https://proceedings.mlr.press/v202/chiang23a.html},
  author    = {Chiang, David and Cholak, Peter and Pillay, Anand},
  title     = {Tighter Bounds on the Expressivity of Transformer Encoders},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages     = {5544--5562}
}

@article{yang2024counting,
  title   = {Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers},
  author  = {Yang, Andy and Chiang, David},
  journal = {arXiv preprint arXiv:2404.04393},
  year    = {2024}
}

@inproceedings{geva-etal-2021-transformer,
  title     = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author    = {Geva, Mor  and
               Schuster, Roei  and
               Berant, Jonathan  and
               Levy, Omer},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.446},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  pages     = {5484--5495},
  abstract  = {Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.}
}

@article{hornik1989multilayer,
  title     = {Multilayer feedforward networks are universal approximators},
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  year      = {1989},
  publisher = {Elsevier}
}

@article{angluin2023masked,
  title   = {Masked hard-attention transformers and boolean RASP recognize exactly the star-free languages},
  author  = {Angluin, Dana and Chiang, David and Yang, Andy},
  journal = {arXiv preprint arXiv:2310.13897},
  year    = {2023}
}

@article{hahn-2020-theoretical,
  title     = {Theoretical limitations of self-attention in neural sequence models},
  author    = {Hahn, Michael},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  pages     = {156--171},
  year      = {2020},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{hao-etal-2022-circuits,
  title     = {Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author    = {Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {10},
  pages     = {800--810},
  year      = {2022},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{merrill-etal-2022-saturated-transformers,
  title     = {Saturated transformers are constant-depth threshold circuits},
  author    = {Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {10},
  pages     = {843--856},
  year      = {2022},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{merrill2020effects,
  title   = {Effects of parameter norm growth during transformer training: Inductive bias from gradient descent},
  author  = {Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah},
  journal = {arXiv preprint arXiv:2010.09697},
  year    = {2020}
}

@inproceedings{xu+:2015,
  title        = {Show, attend and tell: Neural image caption generation with visual attention},
  author       = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle    = {International conference on machine learning},
  pages        = {2048--2057},
  year         = {2015},
  organization = {PMLR}
}

@article{kazemnejad2024impact,
  title   = {The impact of positional encoding on length generalization in transformers},
  author  = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{shaw2018self,
  title   = {Self-attention with relative position representations},
  author  = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal = {arXiv preprint arXiv:1803.02155},
  year    = {2018}
}


@inproceedings{shiv2019,
  author    = {Shiv, Vighnesh and Quirk, Chris},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Novel positional encodings to enable tree-based transformers},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@article{merrill2023parallelism,
  title     = {The parallelism tradeoff: Limitations of log-precision transformers},
  author    = {Merrill, William and Sabharwal, Ashish},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {11},
  pages     = {531--545},
  year      = {2023},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{strobl2024transformers,
  title   = {Transformers as transducers},
  author  = {Strobl, Lena and Angluin, Dana and Chiang, David and Rawski, Jonathan and Sabharwal, Ashish},
  journal = {arXiv preprint arXiv:2404.02040},
  year    = {2024}
}

@inproceedings{press2022train,
  title     = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author    = {Ofir Press and Noah Smith and Mike Lewis},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=R8sQPpGCv0}
}

@misc{huang2020improvetransformermodelsbetter,
  title         = {Improve Transformer Models with Better Relative Position Embeddings},
  author        = {Zhiheng Huang and Davis Liang and Peng Xu and Bing Xiang},
  year          = {2020},
  eprint        = {2009.13658},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2009.13658}
}

@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{zhang2019root,
  title   = {Root mean square layer normalization},
  author  = {Zhang, Biao and Sennrich, Rico},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@inproceedings{brody2023expressivity,
  title     = {On the Expressivity Role of LayerNorm in Transformersâ€™ Attention},
  author    = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {14211--14221},
  year      = {2023}
}

@article{ba2016layer,
  title   = {Layer normalization},
  author  = {Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {ArXiv e-prints},
  pages   = {arXiv--1607},
  year    = {2016}
}

@inproceedings{xiong2020layer,
  title        = {On layer normalization in the transformer architecture},
  author       = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle    = {International Conference on Machine Learning},
  pages        = {10524--10533},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{hahn-rofin-2024-sensitive,
  title     = {Why are Sensitive Functions Hard for Transformers?},
  author    = {Hahn, Michael  and
               Rofin, Mark},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.acl-long.800},
  doi       = {10.18653/v1/2024.acl-long.800},
  pages     = {14973--15008},
  abstract  = {Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers{'} inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.}
}
@article{mielke2021between,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{merrill-2024-little-depth,
  title={A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers},
  author={William Merrill and Ashish Sabharwal},
  booktitle={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=njycONK0JG}
}

@inproceedings{yao-2021-self-attention,
    title = "Self-Attention Networks Can Process Bounded Hierarchical Languages",
    author = "Yao, Shunyu  and
      Peng, Binghui  and
      Papadimitriou, Christos  and
      Narasimhan, Karthik",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.292",
    doi = "10.18653/v1/2021.acl-long.292",
    pages = "3770--3785",
    abstract = "Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.",
}